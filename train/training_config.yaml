# Which pretrained models to finetune
# BASE_MODEL: "bert-base-arabic"                    # 111M, AutoModelForMaskedLM
# BASE_MODEL: "gpt2"                                # 137M
# BASE_MODEL: "mt5-small"                           # 300M 13GB with BS 4 and GACC 16 and BF16, hyperparameters from https://aclanthology.org/2021.findings-acl.413.pdf and https://aclanthology.org/P18-1031.pdf. Though we reduce lr by /10 as we observe some instabilities
BASE_MODEL: "mt5-base"                            # 582M
# BASE_MODEL: "Qwen2.5-0.5B-SFT"                    # 494M
# BASE_MODEL: "Falcon3-1B-Base-SFT"                 # 1B

# Instruct models
# BASE_MODEL: "Qwen2.5-0.5B-Instruct"               # 494M
# BASE_MODEL: "Falcon3-1B-Instruct"                 # 1B
# BASE_MODEL: "Qwen2.5-3B-Instruct"                 # 3B
# BASE_MODEL: "Falcon3-3B-Instruct"

# Dataset to use
DATASET_PATH: BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Filtered # We use the dataset filtered out from samples containing chinese characters "BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset"

# Training hyperparameters
hyperparameters:
    num_train_epochs: 3                 # as after 2 we saw a small bump in loss (decreasing)
    lr: 0.001                           # usually 1e-4 is recommended for Qwen models but as we have a small dataset, we prefer to go slowlier
    batch_size: 2                       # 2 for 0.5B, 1 for 1B, and 2 for 3B LoRA
    gradient_accumulation_steps: 16     # 16 for 0.5B, 32 for 1B
    # eval_accumulation_steps: 3        # to avoid OOM in eval. Slows down eval as it offloads to CPU.
    max_grad_norm: 1.0
    warmup_steps: 500
    warmup_ratio: 0.1

    # LoRA
    USE_LORA: False #False True
    lora_r: 256
    lora_alpha: 128
    lora_dropout: 0.05
    target_modules: 
        - "q_proj"
        - "k_proj"
        - "v_proj"
        - "o_proj"

    # Logging and saving
    logging_steps: 10
    save_steps: 50
    eval_steps: 50

    optimizer: "adamw_torch_fused" # uses less memory. "adamw_torch" and "adamw_torch_fused" for > 1B models to fit in < 15GB VRAM  
    MAX_LEN: 1024 # 1024 or 2048. For 0.5B 2048, and 1024 for > 1B model to fit in < 15GB VRAM

# To observe how the model is performing when we increase the number of training samples
MAX_TRAINING_SAMPLES: 5000 #5000

# Seed for reproducibility
SEED: 42

# metric that indicates best model
METRIC_FOR_BEST_MODEL: "rougeLsum" # as it is particularly useful for evaluating multi-sentence summaries or documents. Indeed, it's a variant of ROUGE-L that splits the text into sentences, computes ROUGE-L for each sentence, and then aggregates the scores.

# precision in training
FP16_TRAINING: True # False

DEFAULT_CHAT_TEMPLATE: "{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'system' %}\n{{ '<|system|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}"

MODELS_DICT:
    bert-base-arabic:
        MODEL_PATH: "asafaya/bert-base-arabic"
        CAUSAL_LM: false
        SFT_TRAINING: false

    gpt2:
        MODEL_PATH: "openai-community/gpt2"
        CAUSAL_LM: true
        SFT_TRAINING: true

    mt5-small:
        MODEL_PATH: "google/mt5-small"
        CAUSAL_LM: false
        SFT_TRAINING: false

    mt5-base:
        MODEL_PATH: "google/mt5-base"
        CAUSAL_LM: false
        SFT_TRAINING: false

    Qwen2.5-0.5B-SFT:
        MODEL_PATH: "Qwen/Qwen2.5-0.5B"
        CAUSAL_LM: true
        SFT_TRAINING: true

    Qwen2.5-0.5B-Instruct:
        MODEL_PATH: "Qwen/Qwen2.5-0.5B-Instruct"
        CAUSAL_LM: true
        SFT_TRAINING: true

    Falcon3-1B-Base-SFT:
        MODEL_PATH: "tiiuae/Falcon3-1B-Base"
        CAUSAL_LM: true
        SFT_TRAINING: true

    Falcon3-1B-Instruct:
        MODEL_PATH: "tiiuae/Falcon3-1B-Instruct"
        CAUSAL_LM: true
        SFT_TRAINING: true

    Qwen2.5-3B-Instruct:
        MODEL_PATH: "Qwen/Qwen2.5-3B-Instruct"
        CAUSAL_LM: true
        SFT_TRAINING: true
    
    Falcon3-3B-Instruct:
        MODEL_PATH: "tiiuae/Falcon3-3B-Instruct"
        CAUSAL_LM: true
        SFT_TRAINING: true