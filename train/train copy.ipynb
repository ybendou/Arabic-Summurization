{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/abounhar/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/infres/abounhar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/infres/abounhar/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/infres/abounhar/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdelazizbounhar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Training configuration:\n",
      "{'BASE_MODEL': 'Qwen2.5-0.5B-Instruct',\n",
      " 'DATASET_PATH': 'BounharAbdelaziz/Arabic-Synthetic-Summarization-Dataset-Filtered',\n",
      " 'FP16_TRAINING': True,\n",
      " 'MAX_TRAINING_SAMPLES': 1000,\n",
      " 'METRIC_FOR_BEST_MODEL': 'rougeL',\n",
      " 'MODELS_DICT': {'Falcon3-1B-Base': {'CAUSAL_LM': True,\n",
      "                                     'MODEL_PATH': 'tiiuae/Falcon3-1B-Base',\n",
      "                                     'SFT_TRAINING': False},\n",
      "                 'Falcon3-1B-Base-SFT': {'CAUSAL_LM': True,\n",
      "                                         'MODEL_PATH': 'tiiuae/Falcon3-1B-Base',\n",
      "                                         'SFT_TRAINING': True},\n",
      "                 'Falcon3-1B-Instruct': {'CAUSAL_LM': True,\n",
      "                                         'MODEL_PATH': 'tiiuae/Falcon3-1B-Instruct',\n",
      "                                         'SFT_TRAINING': True},\n",
      "                 'Falcon3-3B-Instruct': {'CAUSAL_LM': True,\n",
      "                                         'MODEL_PATH': 'tiiuae/Falcon3-3B-Instruct',\n",
      "                                         'SFT_TRAINING': True},\n",
      "                 'MINE': {'CAUSAL_LM': True,\n",
      "                          'MODEL_PATH': 'BounharAbdelaziz/Falcon3-1B-Instruct-bs-1-lr-2e-05-ep-3-wmp-100-gacc-32-gnorm-1.0-FP16-SFT-mxln-1024',\n",
      "                          'SFT_TRAINING': True},\n",
      "                 'Qwen2.5-0.5B': {'CAUSAL_LM': True,\n",
      "                                  'MODEL_PATH': 'Qwen/Qwen2.5-0.5B',\n",
      "                                  'SFT_TRAINING': False},\n",
      "                 'Qwen2.5-0.5B-Instruct': {'CAUSAL_LM': True,\n",
      "                                           'MODEL_PATH': 'Qwen/Qwen2.5-0.5B-Instruct',\n",
      "                                           'SFT_TRAINING': True},\n",
      "                 'Qwen2.5-0.5B-SFT': {'CAUSAL_LM': True,\n",
      "                                      'MODEL_PATH': 'Qwen/Qwen2.5-0.5B',\n",
      "                                      'SFT_TRAINING': True},\n",
      "                 'Qwen2.5-3B-Instruct': {'CAUSAL_LM': True,\n",
      "                                         'MODEL_PATH': 'Qwen/Qwen2.5-3B-Instruct',\n",
      "                                         'SFT_TRAINING': True},\n",
      "                 'bert-base-arabic': {'CAUSAL_LM': False,\n",
      "                                      'MODEL_PATH': 'asafaya/bert-base-arabic',\n",
      "                                      'SFT_TRAINING': False},\n",
      "                 'gpt2': {'CAUSAL_LM': True,\n",
      "                          'MODEL_PATH': 'openai-community/gpt2',\n",
      "                          'SFT_TRAINING': False},\n",
      "                 'mt5-base': {'CAUSAL_LM': False,\n",
      "                              'MODEL_PATH': 'google/mt5-base',\n",
      "                              'SFT_TRAINING': False},\n",
      "                 'mt5-base-SFT': {'CAUSAL_LM': False,\n",
      "                                  'MODEL_PATH': 'google/mt5-base',\n",
      "                                  'SFT_TRAINING': True},\n",
      "                 'mt5-small': {'CAUSAL_LM': False,\n",
      "                               'MODEL_PATH': 'google/mt5-small',\n",
      "                               'SFT_TRAINING': False},\n",
      "                 'mt5-small-SFT': {'CAUSAL_LM': False,\n",
      "                                   'MODEL_PATH': 'google/mt5-small',\n",
      "                                   'SFT_TRAINING': True}},\n",
      " 'SEED': 42,\n",
      " 'hyperparameters': {'MAX_LEN': 1024,\n",
      "                     'USE_LORA': False,\n",
      "                     'batch_size': 2,\n",
      "                     'eval_steps': 100,\n",
      "                     'gradient_accumulation_steps': 32,\n",
      "                     'logging_steps': 50,\n",
      "                     'lora_alpha': 32,\n",
      "                     'lora_dropout': 0.05,\n",
      "                     'lora_r': 128,\n",
      "                     'lr': 5e-06,\n",
      "                     'max_grad_norm': 1.0,\n",
      "                     'num_train_epochs': 1,\n",
      "                     'optimizer': 'adamw_torch_fused',\n",
      "                     'save_steps': 100,\n",
      "                     'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
      "                     'warmup_ratio': 0.04,\n",
      "                     'warmup_steps': 100}}\n",
      "--------------------------------------------------\n",
      "[INFO] Truncating training samples to: 1000...\n",
      "[INFO] Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'summary', 'summary_model_name', 'tokenizer_name', 'dataset_source', 'sequence_length'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'summary', 'summary_model_name', 'tokenizer_name', 'dataset_source', 'sequence_length'],\n",
      "        num_rows: 437\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'summary', 'summary_model_name', 'tokenizer_name', 'dataset_source', 'sequence_length'],\n",
      "        num_rows: 444\n",
      "    })\n",
      "})\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model and Tokenizer loaded: Qwen/Qwen2.5-0.5B-Instruct, version: Qwen2.5-0.5B-Instruct, IS_SFT_TRAINING: True, FP16_TRAINING: True\n",
      "--------------------------------------------------\n",
      "Configuration saved to ./run_configs/meh.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/infres/abounhar/mbzuai/train/wandb/run-20250127_122336-r1qqiwvk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdelazizbounhar/arabic-summarization-v2/runs/r1qqiwvk' target=\"_blank\">meh</a></strong> to <a href='https://wandb.ai/abdelazizbounhar/arabic-summarization-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdelazizbounhar/arabic-summarization-v2' target=\"_blank\">https://wandb.ai/abdelazizbounhar/arabic-summarization-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdelazizbounhar/arabic-summarization-v2/runs/r1qqiwvk' target=\"_blank\">https://wandb.ai/abdelazizbounhar/arabic-summarization-v2/runs/r1qqiwvk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/abdelazizbounhar/arabic-summarization-v2/runs/r1qqiwvk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x745408658b30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import wandb\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    # DataCollatorWithPadding,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "from utils import(\n",
    "    preprocess_function_seq2seq,\n",
    "    preprocess_function_causal_lm,\n",
    "    preprocess_function_causal_lm_sft_training,\n",
    "    preprocess_function_causal_lm_sft_testing,\n",
    "    preprocess_logits_for_metrics,\n",
    "    compute_metrics,\n",
    "    compute_metrics_causal_lm,\n",
    "    set_seed,\n",
    "    print_trainable_params_info,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "# Set up logging and tracking\n",
    "wandb.login()\n",
    "\n",
    "# get training configuration\n",
    "with open('training_config.yaml') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "print('-'*50)\n",
    "print(\"Training configuration:\")\n",
    "pprint(config)\n",
    "print('-'*50)\n",
    "\n",
    "MODELS_DICT = config['MODELS_DICT']\n",
    "\n",
    "# Training hyperparameters\n",
    "num_train_epochs = config['hyperparameters']['num_train_epochs']\n",
    "lr = config['hyperparameters']['lr']\n",
    "batch_size = config['hyperparameters']['batch_size']\n",
    "gradient_accumulation_steps = config['hyperparameters']['gradient_accumulation_steps']\n",
    "max_grad_norm = config['hyperparameters']['max_grad_norm']\n",
    "warmup_steps = config['hyperparameters']['warmup_steps']\n",
    "warmup_ratio = config['hyperparameters']['warmup_ratio']\n",
    "\n",
    "# Logging and saving\n",
    "logging_steps = config['hyperparameters']['logging_steps']\n",
    "save_steps = config['hyperparameters']['save_steps']\n",
    "eval_steps = config['hyperparameters']['eval_steps']\n",
    "\n",
    "# Training data path\n",
    "TRAIN_DATA_PATH = config['DATASET_PATH']\n",
    "\n",
    "# base model path\n",
    "BASE_MODEL = config['BASE_MODEL']\n",
    "MODEL_PATH = MODELS_DICT[BASE_MODEL]['MODEL_PATH']\n",
    "IS_CAUSAL_LM = MODELS_DICT[BASE_MODEL]['CAUSAL_LM']\n",
    "IS_SFT_TRAINING = MODELS_DICT[BASE_MODEL]['SFT_TRAINING']\n",
    "FP16_TRAINING = config['FP16_TRAINING']\n",
    "\n",
    "# max training samples\n",
    "MAX_TRAINING_SAMPLES = config['MAX_TRAINING_SAMPLES']\n",
    "\n",
    "if FP16_TRAINING:\n",
    "    torch_dtype=torch.bfloat16 # bfloat16 has better precission than float16 thanks to bigger mantissa. Though not available with all GPUs architecture.\n",
    "else:\n",
    "    torch_dtype=torch.float32\n",
    "\n",
    "# set seed\n",
    "SEED = config['SEED']\n",
    "set_seed(SEED)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(TRAIN_DATA_PATH)  # Replace with your dataset path\n",
    "\n",
    "# truncate training dataset to observe data size impact on performance\n",
    "print(f'[INFO] Truncating training samples to: {MAX_TRAINING_SAMPLES}...')\n",
    "dataset['train'] = dataset['train'].select(range(min(len(dataset['train']), MAX_TRAINING_SAMPLES)))\n",
    "print(f'[INFO] Dataset loaded: {dataset}')\n",
    "print('-'*50)\n",
    "\n",
    "# Load tokenizer and model\n",
    "if IS_CAUSAL_LM:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch_dtype, \n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "if config['hyperparameters']['USE_LORA']:\n",
    "    # Apply LoRA\n",
    "    print(f\"[INFO] Training with LoRA\")\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "    \n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=config['hyperparameters']['lora_r'],\n",
    "        lora_alpha=config['hyperparameters']['lora_alpha'],\n",
    "        lora_dropout=config['hyperparameters']['lora_dropout'],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\" if IS_CAUSAL_LM else \"SEQ_2_SEQ_LM\",  # Adjust for your task\n",
    "        target_modules=config['hyperparameters']['target_modules'],  # Specify target modules if required\n",
    "    )\n",
    "    \n",
    "    # Wrap the model with LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Log trainable parameters for verification\n",
    "    print_trainable_params_info(model)\n",
    "\n",
    "    print('-'*50)\n",
    "\n",
    "# Set a maximum length for tokenization\n",
    "tokenizer.model_max_length = config['hyperparameters']['MAX_LEN']\n",
    "if BASE_MODEL == \"gpt2\":\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "print(f'[INFO] Model and Tokenizer loaded: {MODEL_PATH}, version: {BASE_MODEL}, IS_SFT_TRAINING: {IS_SFT_TRAINING}, FP16_TRAINING: {FP16_TRAINING}')\n",
    "print('-'*50)\n",
    "\n",
    "# Project name for loggings and savings\n",
    "project_name = \"arabic-summarization-v2\"\n",
    "fp16 = '-FP16' if FP16_TRAINING else ''\n",
    "sft = '-SFT' if IS_SFT_TRAINING else ''\n",
    "# LoRA params\n",
    "lora_training = f'-lora' if config['hyperparameters']['USE_LORA'] else ''\n",
    "lora_r = f'-r-{config['hyperparameters']['lora_r']}' if config['hyperparameters']['USE_LORA'] else ''\n",
    "lora_alpha = f'-a-{config['hyperparameters']['lora_alpha']}' if config['hyperparameters']['USE_LORA'] else ''\n",
    "lora_dropout = f'-d-{config['hyperparameters']['lora_dropout']}' if config['hyperparameters']['USE_LORA'] else ''\n",
    "\n",
    "run_name = 'meh' #f'{MODEL_PATH.split(\"/\")[-1]}-bs-{batch_size}-lr-{lr}-ep-{num_train_epochs}-wp-{warmup_steps}-gacc-{gradient_accumulation_steps}-gnm-{max_grad_norm}{fp16}{sft}-mx-{config['hyperparameters']['MAX_LEN']}{lora_training}{lora_r}{lora_alpha}-v2'\n",
    "assert '--' not in run_name, f\"[WARN] Detected -- in run_name. This will cause a push_to_hub error! Found run_name={run_name} \"\n",
    "assert len(run_name) < 96, f\"[WARN] run_name too long. This will cause a push_to_hub error! Consider squeezing it. Found run_name={run_name}\"\n",
    "\n",
    "# Where to save the model\n",
    "MODEL_RUN_SAVE_PATH = f\"BounharAbdelaziz/{run_name}\"\n",
    "\n",
    "# Save the configuration to a .txt file\n",
    "output_filename = f\"./run_configs/{run_name}.txt\"\n",
    "with open(output_filename, 'w') as output_file:\n",
    "    for key, value in config.items():\n",
    "        output_file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(f\"Configuration saved to {output_filename}\")\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged, all runs will be under this project\n",
    "    project=project_name,   \n",
    "    # Group runs by model size\n",
    "    group=MODEL_PATH,       \n",
    "    # Unique run name\n",
    "    name=run_name,\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_train_epochs\": num_train_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"warmup_ratio\": warmup_ratio,\n",
    "        # \"warmup_steps\": warmup_steps,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "        # \"weight_decay\": weight_decay,\n",
    "        \"dataset\": TRAIN_DATA_PATH,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 7017.39 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 437/437 [00:00<00:00, 6436.39 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1306.58 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 437/437 [00:00<00:00, 1324.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# custom instruct prompt start\n",
    "prompt_template = f\"Summarize this arabic text:\\n{{text}}\\n---\\nSummary:\\n{{summary}}{{eos_token}}\"\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = prompt_template.format(text=sample[\"text\"],\n",
    "                                            summary=sample[\"summary\"],\n",
    "                                            eos_token=tokenizer.eos_token)\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "train_dataset = dataset[\"train\"].map(template_dataset)\n",
    "eval_dataset = dataset[\"validation\"].map(template_dataset)\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_train_dataset = train_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(train_dataset.features)\n",
    ")\n",
    "\n",
    "\n",
    "lm_eval_dataset = eval_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(eval_dataset.features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/abounhar/.local/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if IS_CAUSAL_LM:\n",
    "    \n",
    "    if IS_SFT_TRAINING:\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=MODEL_RUN_SAVE_PATH,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            learning_rate=lr,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_total_limit=1,\n",
    "            bf16=config['FP16_TRAINING'],\n",
    "            fp16_full_eval=config['FP16_TRAINING'],\n",
    "            logging_steps=logging_steps,\n",
    "            save_steps=save_steps,\n",
    "            eval_steps=eval_steps,\n",
    "            report_to=\"wandb\",\n",
    "            push_to_hub=False,\n",
    "            metric_for_best_model=config['METRIC_FOR_BEST_MODEL'],\n",
    "            gradient_checkpointing=True,\n",
    "            load_best_model_at_end=True,\n",
    "            optim=config['hyperparameters']['optimizer'],\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": False} if config['hyperparameters']['USE_LORA'] else None,  # Avoids gradient issues in backprop when LoRA is set to True. # https://discuss.huggingface.co/t/how-to-combine-lora-and-gradient-checkpointing-in-whisper/50629\n",
    "        )\n",
    "        \n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=lm_train_dataset,\n",
    "            eval_dataset=lm_eval_dataset,\n",
    "            data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "            compute_metrics=lambda x : compute_metrics_causal_lm(x, tokenizer),\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics, # avoids OOM in eval\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(f'[INFO] Running preprocess_function_causal_lm')\n",
    "        # Apply preprocessing\n",
    "        tokenized_dataset_train = dataset['train'].map(lambda x: preprocess_function_causal_lm(x, tokenizer), batched=True)\n",
    "        tokenized_dataset_validation = dataset['validation'].map(lambda x: preprocess_function_causal_lm(x, tokenizer), batched=True)\n",
    "        tokenized_dataset_test = dataset['test'].map(lambda x: preprocess_function_causal_lm(x, tokenizer), batched=True)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=MODEL_RUN_SAVE_PATH,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            learning_rate=lr,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_total_limit=1,\n",
    "            bf16=config['FP16_TRAINING'],\n",
    "            fp16_full_eval=config['FP16_TRAINING'],\n",
    "            logging_steps=logging_steps,\n",
    "            save_steps=save_steps,\n",
    "            eval_steps=eval_steps,\n",
    "            report_to=\"wandb\",\n",
    "            push_to_hub=False,\n",
    "            metric_for_best_model=config['METRIC_FOR_BEST_MODEL'],\n",
    "            gradient_checkpointing=True,\n",
    "            load_best_model_at_end=True,\n",
    "            optim=config['hyperparameters']['optimizer'],\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": False} if config['hyperparameters']['USE_LORA'] else None,  # Avoids gradient issues in backprop when LoRA is set to True. # https://discuss.huggingface.co/t/how-to-combine-lora-and-gradient-checkpointing-in-whisper/50629\n",
    "        )\n",
    "    \n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset_train,\n",
    "            eval_dataset=tokenized_dataset_validation,\n",
    "            data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "            compute_metrics=lambda x : compute_metrics_causal_lm(x, tokenizer),\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics, # avoids OOM in eval\n",
    "        )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    tokenized_dataset_train = dataset['train'].map(lambda x: preprocess_function_seq2seq(x, tokenizer), batched=True)\n",
    "    tokenized_dataset_validation = dataset['validation'].map(lambda x: preprocess_function_seq2seq(x, tokenizer), batched=True)\n",
    "    tokenized_dataset_test = dataset['test'].map(lambda x: preprocess_function_seq2seq(x, tokenizer), batched=True)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=MODEL_RUN_SAVE_PATH,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        learning_rate=lr,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        save_total_limit=1,\n",
    "        predict_with_generate=True,\n",
    "        logging_steps=logging_steps,\n",
    "        save_steps=save_steps,\n",
    "        eval_steps=eval_steps,\n",
    "        report_to=\"wandb\",\n",
    "        push_to_hub=False,\n",
    "        metric_for_best_model=config['METRIC_FOR_BEST_MODEL'],\n",
    "        gradient_checkpointing=True,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset_train,\n",
    "        eval_dataset=tokenized_dataset_validation,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "        compute_metrics=lambda x : compute_metrics(x, tokenizer),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 07:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Bertscore Precision</th>\n",
       "      <th>Bertscore Recall</th>\n",
       "      <th>Bertscore F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.582500</td>\n",
       "      <td>2.602884</td>\n",
       "      <td>55.083788</td>\n",
       "      <td>38.448823</td>\n",
       "      <td>54.245476</td>\n",
       "      <td>54.241797</td>\n",
       "      <td>3.810823</td>\n",
       "      <td>18.660133</td>\n",
       "      <td>67.762839</td>\n",
       "      <td>70.961604</td>\n",
       "      <td>69.278592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.529300</td>\n",
       "      <td>2.587335</td>\n",
       "      <td>55.606804</td>\n",
       "      <td>38.910257</td>\n",
       "      <td>54.759650</td>\n",
       "      <td>54.720753</td>\n",
       "      <td>3.871437</td>\n",
       "      <td>18.798846</td>\n",
       "      <td>67.902154</td>\n",
       "      <td>71.064663</td>\n",
       "      <td>69.402197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.582000</td>\n",
       "      <td>2.582069</td>\n",
       "      <td>55.405143</td>\n",
       "      <td>38.879908</td>\n",
       "      <td>54.549638</td>\n",
       "      <td>54.533909</td>\n",
       "      <td>3.940570</td>\n",
       "      <td>18.912171</td>\n",
       "      <td>67.831755</td>\n",
       "      <td>71.012572</td>\n",
       "      <td>69.341729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.547800</td>\n",
       "      <td>2.580348</td>\n",
       "      <td>55.257497</td>\n",
       "      <td>38.818235</td>\n",
       "      <td>54.404744</td>\n",
       "      <td>54.371896</td>\n",
       "      <td>3.913501</td>\n",
       "      <td>18.850758</td>\n",
       "      <td>67.641649</td>\n",
       "      <td>70.923031</td>\n",
       "      <td>69.194091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.551100</td>\n",
       "      <td>2.580014</td>\n",
       "      <td>55.278462</td>\n",
       "      <td>38.795217</td>\n",
       "      <td>54.417087</td>\n",
       "      <td>54.376928</td>\n",
       "      <td>3.914574</td>\n",
       "      <td>18.879967</td>\n",
       "      <td>67.650611</td>\n",
       "      <td>70.893994</td>\n",
       "      <td>69.188069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=2.569189453125, metrics={'train_runtime': 468.8698, 'train_samples_per_second': 2.133, 'train_steps_per_second': 1.066, 'total_flos': 3942051575514624.0, 'train_loss': 2.569189453125, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preparing to push to hub...\n"
     ]
    }
   ],
   "source": [
    "# Push to Hugging Face Hub\n",
    "print(\"[INFO] Preparing to push to hub...\")\n",
    "\n",
    "if config['hyperparameters']['USE_LORA']:\n",
    "    print(\"[INFO] Merging LoRA weights before pushing...\")\n",
    "    from peft import merge_and_unload\n",
    "    model = merge_and_unload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Pushing model and tokenizer to Hugging Face Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'BounharAbdelaziz/meh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Pushing model and tokenizer to Hugging Face Hub...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m trainer\u001b[38;5;241m.\u001b[39mpush_to_hub()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_RUN_SAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/hub.py:938\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_repo(\n\u001b[1;32m    934\u001b[0m     repo_id, private\u001b[38;5;241m=\u001b[39mprivate, token\u001b[38;5;241m=\u001b[39mtoken, repo_url\u001b[38;5;241m=\u001b[39mrepo_url, organization\u001b[38;5;241m=\u001b[39morganization\n\u001b[1;32m    935\u001b[0m )\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# Create a new empty model card and eventually tag it\u001b[39;00m\n\u001b[0;32m--> 938\u001b[0m model_card \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_and_tag_model_card\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_metadata_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_errors\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_temp_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    943\u001b[0m     use_temp_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(working_dir)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/hub.py:1193\u001b[0m, in \u001b[0;36mcreate_and_tag_model_card\u001b[0;34m(repo_id, tags, token, ignore_metadata_errors)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;124;03mCreates or loads an existing model card and tags it.\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m        the process. Use it at your own risk.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;66;03m# Check if the model card is present on the remote repo\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     model_card \u001b[38;5;241m=\u001b[39m \u001b[43mModelCard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_metadata_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;66;03m# Otherwise create a simple model card from template\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m     model_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the model card of a ðŸ¤— transformers model that has been pushed on the Hub. This model card has been automatically generated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/huggingface_hub/repocard.py:189\u001b[0m, in \u001b[0;36mRepoCard.load\u001b[0;34m(cls, repo_id_or_path, repo_type, token, ignore_metadata_errors)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load RepoCard: path not found on disk (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Preserve newlines in the existing file.\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mcard_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(f\u001b[38;5;241m.\u001b[39mread(), ignore_metadata_errors\u001b[38;5;241m=\u001b[39mignore_metadata_errors)\n",
      "File \u001b[0;32m/usr/lib/python3.12/pathlib.py:1015\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1014\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1015\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'BounharAbdelaziz/meh'"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer locally before pushing\n",
    "# trainer.save_model(MODEL_RUN_SAVE_PATH)  # This saves the model, tokenizer, and config\n",
    "# tokenizer.save_pretrained(MODEL_RUN_SAVE_PATH)\n",
    "\n",
    "# Push to the hub\n",
    "print(\"[INFO] Pushing model and tokenizer to Hugging Face Hub...\")\n",
    "trainer.push_to_hub()\n",
    "tokenizer.push_to_hub(MODEL_RUN_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize this arabic text:\n",
      ",ØªØ´ÙŠØ± Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ…Ø§Ø±ÙŠÙ† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø§Ù„ÙØªØ±ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ù…Ø³ØªÙˆÙŠ\n",
      "\"Ø¹Ø¨Ø¯ Ø§Ù„Ø­ÙƒÙŠÙ… Ø­Ø°Ø§Ù‚Ø©-Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± Ø¹Ø§Ø´Øª Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± Ù…Ø±Ø­Ù„Ø© Ù…ÙØµÙ„ÙŠØ© ÙÙŠ ØªØ§Ø±ÙŠØ®Ù‡Ø§ Ø®Ù„Ø§Ù„ Ø§Ù„Ø´Ù‡ÙˆØ±\n",
      "Ø§Ù„Ø¹Ø´Ø±Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ù…Ù† Ø¹Ø§Ù… 2019ØŒ Ø´Ù‡Ø¯Øª Ø®Ù„Ø§Ù„Ù‡Ø§ Ø«ÙˆØ±Ø© Ø³Ù„Ù…ÙŠØ© ØºÙŠØ± Ù…Ø³Ø¨ÙˆÙ‚Ø©ØŒ ÙƒØ§Ù†Øª ÙÙŠÙ‡Ø§ Ù‚ÙˆØ§Øª\n",
      "Ø§Ù„Ø¬ÙŠØ´ Ù…Ø±Ø§ÙÙ‚Ø© Ù„Ø­Ø±Ø§Ùƒ Ø§Ù„Ø´Ø¹Ø¨ØŒ Ø¯ÙˆÙ† Ø£Ù† ØªØ±Ø§Ù‚ Ù‚Ø·Ø±Ø© Ø¯Ù…. ÙˆØ£Ø·Ø§Ø­Øª Ø§Ù„Ø«ÙˆØ±Ø© Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ Ø¹Ø¨Ø¯\n",
      "Ø§Ù„Ø¹Ø²ÙŠØ² Ø¨ÙˆØªÙÙ„ÙŠÙ‚Ø© Ø¨Ø¹Ø¯ Ø¹Ø´Ø±ÙŠÙ† Ø¹Ø§Ù…Ø§ Ù…Ù† Ø§Ù„Ø­ÙƒÙ…ØŒ ÙˆØ²Ø¬ Ø¨Ø£Ù‚ÙˆÙ‰ Ø±Ù…ÙˆØ² Ø¹Ù‡Ø¯Ù‡ Ù…Ù† Ù…Ø³Ø¤ÙˆÙ„ÙŠÙ† Ø£Ù…Ù†ÙŠÙŠÙ†\n",
      "ÙˆØ­ÙƒÙˆÙ…ÙŠÙŠÙ† ÙˆØ±Ø¬Ø§Ù„ Ù…Ø§Ù„ ÙˆØ±Ø§Ø¡ Ø§Ù„Ù‚Ø¶Ø¨Ø§Ù†ØŒ Ø­ÙŠØ« Ø´Ù‡Ø¯Øª Ø§Ù„Ø¨Ù„Ø§Ø¯ Ù…Ø­Ø§ÙƒÙ…Ø§Øª ØªØ§Ø±ÙŠØ®ÙŠØ© Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© ÙÙŠ Ø­Ù‚\n",
      "Ù‡Ø¤Ù„Ø§Ø¡. ÙˆØ§Ù†ØªÙ‡Ù‰ Ø§Ù„Ù…Ø®Ø§Ø¶ Ø§Ù„Ø¹Ø³ÙŠØ± Ø·ÙŠÙ„Ø© Ø¹Ø´Ø±Ø© Ø£Ø´Ù‡Ø± Ø¨Ø§Ù†ØªØ®Ø§Ø¨ Ø±Ø¦ÙŠØ³ Ø¬Ø¯ÙŠØ¯ ÙÙŠ 12\n",
      "Ø¯ÙŠØ³Ù…Ø¨Ø±/ÙƒØ§Ù†ÙˆÙ† Ø§Ù„Ø£ÙˆÙ„ØŒ ÙˆØ³Ø· Ø£Ø¬ÙˆØ§Ø¡ Ù…Ø´Ø­ÙˆÙ†Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¤ÙŠØ¯ÙŠÙ† ÙˆØ§Ù„Ø±Ø§ÙØ¶ÙŠÙ† Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ø§Ù†ØªØ®Ø§Ø¨Ø§ØªØŒ\n",
      "Ø­ÙŠØ« Ù„Ø§ ÙŠØ²Ø§Ù„ Ø¬Ø²Ø§Ø¦Ø±ÙŠÙˆÙ† Ù…ØµØ±ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø­ØªØ¬Ø§Ø¬ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ÙŠ. ÙˆÙØªØ­Øª Ø§Ù„ÙˆÙØ§Ø© Ø§Ù„Ù…ÙØ§Ø¬Ø¦Ø© Ù„Ø±Ø¦ÙŠØ³\n",
      "Ø§Ù„Ø£Ø±ÙƒØ§Ù† Ø§Ù„ÙØ±ÙŠÙ‚ Ø£Ø­Ù…Ø¯ Ù‚Ø§ÙŠØ¯ ØµØ§Ù„Ø­ ÙŠÙˆÙ… 23 Ø¯ÙŠØ³Ù…Ø¨Ø±/ÙƒØ§Ù†ÙˆÙ† Ø§Ù„Ø£ÙˆÙ„ Ø¨Ø§Ø¨ Ø§Ù„ØªÙƒÙ‡Ù†Ø§Øª Ø­ÙˆÙ„ Ù…Ù„Ø§Ù…Ø­\n",
      "Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯ØŒ Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø¯ÙˆØ± Ø§Ù„Ø±Ø¬Ù„ Ø§Ù„Ø­Ø§Ø³Ù… ÙÙŠ ØµÙ†Ø§Ø¹Ø© Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ù…Ø§Ø¶ÙŠØ©.\n",
      "\n",
      "---\n",
      "Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_input = \"\"\",ØªØ´ÙŠØ± Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ…Ø§Ø±ÙŠÙ† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø§Ù„ÙØªØ±ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ù…Ø³ØªÙˆÙŠ\n",
    "\"Ø¹Ø¨Ø¯ Ø§Ù„Ø­ÙƒÙŠÙ… Ø­Ø°Ø§Ù‚Ø©-Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± Ø¹Ø§Ø´Øª Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± Ù…Ø±Ø­Ù„Ø© Ù…ÙØµÙ„ÙŠØ© ÙÙŠ ØªØ§Ø±ÙŠØ®Ù‡Ø§ Ø®Ù„Ø§Ù„ Ø§Ù„Ø´Ù‡ÙˆØ±\n",
    "Ø§Ù„Ø¹Ø´Ø±Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ù…Ù† Ø¹Ø§Ù… 2019ØŒ Ø´Ù‡Ø¯Øª Ø®Ù„Ø§Ù„Ù‡Ø§ Ø«ÙˆØ±Ø© Ø³Ù„Ù…ÙŠØ© ØºÙŠØ± Ù…Ø³Ø¨ÙˆÙ‚Ø©ØŒ ÙƒØ§Ù†Øª ÙÙŠÙ‡Ø§ Ù‚ÙˆØ§Øª\n",
    "Ø§Ù„Ø¬ÙŠØ´ Ù…Ø±Ø§ÙÙ‚Ø© Ù„Ø­Ø±Ø§Ùƒ Ø§Ù„Ø´Ø¹Ø¨ØŒ Ø¯ÙˆÙ† Ø£Ù† ØªØ±Ø§Ù‚ Ù‚Ø·Ø±Ø© Ø¯Ù…. ÙˆØ£Ø·Ø§Ø­Øª Ø§Ù„Ø«ÙˆØ±Ø© Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ Ø¹Ø¨Ø¯\n",
    "Ø§Ù„Ø¹Ø²ÙŠØ² Ø¨ÙˆØªÙÙ„ÙŠÙ‚Ø© Ø¨Ø¹Ø¯ Ø¹Ø´Ø±ÙŠÙ† Ø¹Ø§Ù…Ø§ Ù…Ù† Ø§Ù„Ø­ÙƒÙ…ØŒ ÙˆØ²Ø¬ Ø¨Ø£Ù‚ÙˆÙ‰ Ø±Ù…ÙˆØ² Ø¹Ù‡Ø¯Ù‡ Ù…Ù† Ù…Ø³Ø¤ÙˆÙ„ÙŠÙ† Ø£Ù…Ù†ÙŠÙŠÙ†\n",
    "ÙˆØ­ÙƒÙˆÙ…ÙŠÙŠÙ† ÙˆØ±Ø¬Ø§Ù„ Ù…Ø§Ù„ ÙˆØ±Ø§Ø¡ Ø§Ù„Ù‚Ø¶Ø¨Ø§Ù†ØŒ Ø­ÙŠØ« Ø´Ù‡Ø¯Øª Ø§Ù„Ø¨Ù„Ø§Ø¯ Ù…Ø­Ø§ÙƒÙ…Ø§Øª ØªØ§Ø±ÙŠØ®ÙŠØ© Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© ÙÙŠ Ø­Ù‚\n",
    "Ù‡Ø¤Ù„Ø§Ø¡. ÙˆØ§Ù†ØªÙ‡Ù‰ Ø§Ù„Ù…Ø®Ø§Ø¶ Ø§Ù„Ø¹Ø³ÙŠØ± Ø·ÙŠÙ„Ø© Ø¹Ø´Ø±Ø© Ø£Ø´Ù‡Ø± Ø¨Ø§Ù†ØªØ®Ø§Ø¨ Ø±Ø¦ÙŠØ³ Ø¬Ø¯ÙŠØ¯ ÙÙŠ 12\n",
    "Ø¯ÙŠØ³Ù…Ø¨Ø±/ÙƒØ§Ù†ÙˆÙ† Ø§Ù„Ø£ÙˆÙ„ØŒ ÙˆØ³Ø· Ø£Ø¬ÙˆØ§Ø¡ Ù…Ø´Ø­ÙˆÙ†Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¤ÙŠØ¯ÙŠÙ† ÙˆØ§Ù„Ø±Ø§ÙØ¶ÙŠÙ† Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ø§Ù†ØªØ®Ø§Ø¨Ø§ØªØŒ\n",
    "Ø­ÙŠØ« Ù„Ø§ ÙŠØ²Ø§Ù„ Ø¬Ø²Ø§Ø¦Ø±ÙŠÙˆÙ† Ù…ØµØ±ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø­ØªØ¬Ø§Ø¬ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ÙŠ. ÙˆÙØªØ­Øª Ø§Ù„ÙˆÙØ§Ø© Ø§Ù„Ù…ÙØ§Ø¬Ø¦Ø© Ù„Ø±Ø¦ÙŠØ³\n",
    "Ø§Ù„Ø£Ø±ÙƒØ§Ù† Ø§Ù„ÙØ±ÙŠÙ‚ Ø£Ø­Ù…Ø¯ Ù‚Ø§ÙŠØ¯ ØµØ§Ù„Ø­ ÙŠÙˆÙ… 23 Ø¯ÙŠØ³Ù…Ø¨Ø±/ÙƒØ§Ù†ÙˆÙ† Ø§Ù„Ø£ÙˆÙ„ Ø¨Ø§Ø¨ Ø§Ù„ØªÙƒÙ‡Ù†Ø§Øª Ø­ÙˆÙ„ Ù…Ù„Ø§Ù…Ø­\n",
    "Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯ØŒ Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø¯ÙˆØ± Ø§Ù„Ø±Ø¬Ù„ Ø§Ù„Ø­Ø§Ø³Ù… ÙÙŠ ØµÙ†Ø§Ø¹Ø© Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ù…Ø§Ø¶ÙŠØ©.\n",
    "\"\"\"\n",
    "# custom instruct prompt start\n",
    "prompt_template_test = f\"Summarize this arabic text:\\n{{text}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "test_prompt = prompt_template_test.format(text=test_input)\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test summary: Summarize this arabic text:\n",
      ",ØªØ´ÙŠØ± Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙ…Ø§Ø±ÙŠÙ† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø§Ù„ÙØªØ±ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ù…Ø³ØªÙˆÙŠ\n",
      "\"Ø¹Ø¨Ø¯ Ø§Ù„Ø­ÙƒÙŠÙ… Ø­Ø°Ø§Ù‚Ø©-Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± Ø¹Ø§Ø´Øª Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± Ù…Ø±Ø­Ù„Ø© Ù…ÙØµÙ„ÙŠØ© ÙÙŠ ØªØ§Ø±ÙŠØ®Ù‡Ø§ Ø®Ù„Ø§Ù„ Ø§Ù„Ø´Ù‡ÙˆØ±\n",
      "Ø§Ù„Ø¹Ø´Ø±Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ù…Ù† Ø¹Ø§Ù… 2019ØŒ Ø´Ù‡Ø¯Øª Ø®Ù„Ø§Ù„Ù‡Ø§ Ø«ÙˆØ±Ø© Ø³Ù„Ù…ÙŠØ© ØºÙŠØ± Ù…Ø³Ø¨ÙˆÙ‚Ø©ØŒ ÙƒØ§Ù†Øª ÙÙŠÙ‡Ø§ Ù‚ÙˆØ§Øª\n",
      "Ø§Ù„Ø¬ÙŠØ´ Ù…Ø±Ø§ÙÙ‚Ø© Ù„Ø­Ø±Ø§Ùƒ Ø§Ù„Ø´Ø¹Ø¨ØŒ Ø¯ÙˆÙ† Ø£Ù† ØªØ±Ø§Ù‚ Ù‚Ø·Ø±Ø© Ø¯Ù…. ÙˆØ£Ø·Ø§Ø­Øª Ø§Ù„Ø«ÙˆØ±Ø© Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ Ø¹Ø¨Ø¯\n",
      "Ø§Ù„Ø¹Ø²ÙŠØ² Ø¨ÙˆØªÙÙ„ÙŠÙ‚Ø© Ø¨Ø¹Ø¯ Ø¹Ø´Ø±ÙŠÙ† Ø¹Ø§Ù…Ø§ Ù…Ù† Ø§Ù„Ø­ÙƒÙ…ØŒ ÙˆØ²Ø¬ Ø¨Ø£Ù‚ÙˆÙ‰ Ø±Ù…ÙˆØ² Ø¹Ù‡Ø¯Ù‡ Ù…Ù† Ù…Ø³Ø¤ÙˆÙ„ÙŠÙ† Ø£Ù…Ù†ÙŠÙŠÙ†\n",
      "ÙˆØ­ÙƒÙˆÙ…ÙŠÙŠÙ† ÙˆØ±Ø¬Ø§Ù„ Ù…Ø§Ù„ ÙˆØ±Ø§Ø¡ Ø§Ù„Ù‚Ø¶Ø¨Ø§Ù†ØŒ Ø­ÙŠØ« Ø´Ù‡Ø¯Øª Ø§Ù„Ø¨Ù„Ø§Ø¯ Ù…Ø­Ø§ÙƒÙ…Ø§Øª ØªØ§Ø±ÙŠØ®ÙŠØ© Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© ÙÙŠ Ø­Ù‚\n",
      "Ù‡Ø¤Ù„Ø§Ø¡. ÙˆØ§Ù†ØªÙ‡Ù‰ Ø§Ù„Ù…Ø®Ø§Ø¶ Ø§Ù„Ø¹Ø³ÙŠØ± Ø·ÙŠÙ„Ø© Ø¹Ø´Ø±Ø© Ø£Ø´Ù‡Ø± Ø¨Ø§Ù†ØªØ®Ø§Ø¨ Ø±Ø¦ÙŠØ³ Ø¬Ø¯ÙŠØ¯ ÙÙŠ 12\n",
      "Ø¯ÙŠØ³Ù…Ø¨Ø±/ÙƒØ§Ù†ÙˆÙ† Ø§Ù„Ø£ÙˆÙ„ØŒ ÙˆØ³Ø· Ø£Ø¬ÙˆØ§Ø¡ Ù…Ø´Ø­ÙˆÙ†Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¤ÙŠØ¯ÙŠÙ† ÙˆØ§Ù„Ø±Ø§ÙØ¶ÙŠÙ† Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ø§Ù†ØªØ®Ø§Ø¨Ø§ØªØŒ\n",
      "Ø­ÙŠØ« Ù„Ø§ ÙŠØ²Ø§Ù„ Ø¬Ø²Ø§Ø¦Ø±ÙŠÙˆÙ† Ù…ØµØ±ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø­ØªØ¬Ø§Ø¬ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ÙŠ. ÙˆÙØªØ­Øª Ø§Ù„ÙˆÙØ§Ø© Ø§Ù„Ù…ÙØ§Ø¬Ø¦Ø© Ù„Ø±Ø¦ÙŠØ³\n",
      "Ø§Ù„Ø£Ø±ÙƒØ§Ù† Ø§Ù„ÙØ±ÙŠÙ‚ Ø£Ø­Ù…Ø¯ Ù‚Ø§ÙŠØ¯ ØµØ§Ù„Ø­ ÙŠÙˆÙ… 23 Ø¯ÙŠØ³Ù…Ø¨Ø±/ÙƒØ§Ù†ÙˆÙ† Ø§Ù„Ø£ÙˆÙ„ Ø¨Ø§Ø¨ Ø§Ù„ØªÙƒÙ‡Ù†Ø§Øª Ø­ÙˆÙ„ Ù…Ù„Ø§Ù…Ø­\n",
      "Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯ØŒ Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø¯ÙˆØ± Ø§Ù„Ø±Ø¬Ù„ Ø§Ù„Ø­Ø§Ø³Ù… ÙÙŠ ØµÙ†Ø§Ø¹Ø© Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ù…Ø§Ø¶ÙŠØ©.\n",
      "\n",
      "---\n",
      "Summary:\n",
      "ØªØ¹Ø±Ø¶Øª Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± Ù„ perÃ­Ø§Ø¯Ø© Ù…Ø­Ø¯Ø¯Ø© Ø®Ù„Ø§Ù„ Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©ØŒ Ù…Ø¹ Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø«\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and generate\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "        **inputs,\n",
    "        # max_new_tokens=256,\n",
    "        # num_beams=4,\n",
    "        # early_stopping=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        # repetition_penalty=1.2,\n",
    "        eos_token_id=tokenizer.eos_token_id,  # Crucial for stopping\n",
    "        # pad_token_id=tokenizer.eos_token_id,  # Crucial for stopping\n",
    "        # do_sample=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# Decode the output\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# split_text = summary.split(\"### Assistant:\", 1)\n",
    "# final_pred = split_text[-1].strip() if len(split_text) > 1 else summary\n",
    "# final_pred = final_pred.replace(tokenizer.eos_token, \"\")\n",
    "        \n",
    "print(f\"Test summary: {summary}\")\n",
    "# print(f\"final_pred: {final_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(\n",
    "    tokenized_dataset_test,\n",
    "    # compute_metrics=lambda eval_pred: compute_metrics_causal_lm(eval_pred, is_in_test=True)\n",
    ")\n",
    "print(f'[INFO] Results on test set: {test_results}')\n",
    "\n",
    "# # Save the model and tokenizer locally before pushing\n",
    "# trainer.save_model(MODEL_RUN_SAVE_PATH)  # This saves the model, tokenizer, and config\n",
    "# tokenizer.save_pretrained(MODEL_RUN_SAVE_PATH)\n",
    "\n",
    "# # Push to the hub\n",
    "# print(\"[INFO] Pushing model and tokenizer to Hugging Face Hub...\")\n",
    "# trainer.push_to_hub()\n",
    "# tokenizer.push_to_hub(MODEL_RUN_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('preds.csv')\n",
    "\n",
    "print(df[\"Predictions\"].iloc[0])\n",
    "\n",
    "print(df[\"Labels\"].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Labels\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
